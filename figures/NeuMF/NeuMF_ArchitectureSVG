digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1690308221184 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	1690186652192 [label=AddmmBackward0]
	1690296435232 -> 1690186652192
	1690308188176 [label="affine_output.bias
 (1)" fillcolor=lightblue]
	1690308188176 -> 1690296435232
	1690296435232 [label=AccumulateGrad]
	1690186648352 -> 1690186652192
	1690186648352 [label=CatBackward0]
	1690186647344 -> 1690186648352
	1690186647344 [label=ReluBackward0]
	1690186653584 -> 1690186647344
	1690186653584 [label=AddmmBackward0]
	1690143032176 -> 1690186653584
	1692215758352 [label="fc_layers.1.bias
 (8)" fillcolor=lightblue]
	1692215758352 -> 1690143032176
	1690143032176 [label=AccumulateGrad]
	1690186645760 -> 1690186653584
	1690186645760 [label=ReluBackward0]
	1690186646288 -> 1690186645760
	1690186646288 [label=AddmmBackward0]
	1690143040048 -> 1690186646288
	1690308191056 [label="fc_layers.0.bias
 (16)" fillcolor=lightblue]
	1690308191056 -> 1690143040048
	1690143040048 [label=AccumulateGrad]
	1690186651664 -> 1690186646288
	1690186651664 [label=CatBackward0]
	1690186659248 -> 1690186651664
	1690186659248 [label=EmbeddingBackward0]
	1690143045040 -> 1690186659248
	1690308183536 [label="embedding_user_mlp.weight
 (100, 16)" fillcolor=lightblue]
	1690308183536 -> 1690143045040
	1690143045040 [label=AccumulateGrad]
	1690186645808 -> 1690186651664
	1690186645808 [label=EmbeddingBackward0]
	1690186164800 -> 1690186645808
	1690308191536 [label="embedding_item_mlp.weight
 (200, 16)" fillcolor=lightblue]
	1690308191536 -> 1690186164800
	1690186164800 [label=AccumulateGrad]
	1690186649792 -> 1690186646288
	1690186649792 [label=TBackward0]
	1690143031984 -> 1690186649792
	1690308191616 [label="fc_layers.0.weight
 (16, 32)" fillcolor=lightblue]
	1690308191616 -> 1690143031984
	1690143031984 [label=AccumulateGrad]
	1690186652672 -> 1690186653584
	1690186652672 [label=TBackward0]
	1690143037504 -> 1690186652672
	1690142472176 [label="fc_layers.1.weight
 (8, 16)" fillcolor=lightblue]
	1690142472176 -> 1690143037504
	1690143037504 [label=AccumulateGrad]
	1690186647104 -> 1690186648352
	1690186647104 [label=MulBackward0]
	1690186660208 -> 1690186647104
	1690186660208 [label=EmbeddingBackward0]
	1690186158752 -> 1690186660208
	1690308191456 [label="embedding_user_mf.weight
 (100, 16)" fillcolor=lightblue]
	1690308191456 -> 1690186158752
	1690186158752 [label=AccumulateGrad]
	1690186653968 -> 1690186647104
	1690186653968 [label=EmbeddingBackward0]
	1690186163792 -> 1690186653968
	1690308191216 [label="embedding_item_mf.weight
 (200, 16)" fillcolor=lightblue]
	1690308191216 -> 1690186163792
	1690186163792 [label=AccumulateGrad]
	1690186650560 -> 1690186652192
	1690186650560 [label=TBackward0]
	1690296448096 -> 1690186650560
	1690308185536 [label="affine_output.weight
 (1, 24)" fillcolor=lightblue]
	1690308185536 -> 1690296448096
	1690296448096 [label=AccumulateGrad]
	1690186652192 -> 1690308221184
}
