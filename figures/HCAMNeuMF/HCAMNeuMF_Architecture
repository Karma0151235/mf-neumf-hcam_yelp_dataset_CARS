digraph {
	graph [size="19.8,19.8"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1685817474048 [label="
 ()" fillcolor=darkolivegreen1]
	1685818555504 [label=SqueezeBackward0]
	1685818544464 -> 1685818555504
	1685818544464 [label=AddmmBackward0]
	1685819049296 -> 1685818544464
	1685818039968 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	1685818039968 -> 1685819049296
	1685819049296 [label=AccumulateGrad]
	1685819061776 -> 1685818544464
	1685819061776 [label=CatBackward0]
	1685819048912 -> 1685819061776
	1685819048912 [label=MulBackward0]
	1685819063264 -> 1685819048912
	1685819063264 [label=EmbeddingBackward0]
	1685793499952 -> 1685819063264
	1685818089200 [label="user_embedding_mf.weight
 (100, 128)" fillcolor=lightblue]
	1685818089200 -> 1685793499952
	1685793499952 [label=AccumulateGrad]
	1685793496880 -> 1685819048912
	1685793496880 [label=EmbeddingBackward0]
	1685793489008 -> 1685793496880
	1685818089120 [label="item_embedding_mf.weight
 (200, 128)" fillcolor=lightblue]
	1685818089120 -> 1685793489008
	1685793489008 [label=AccumulateGrad]
	1685819059472 -> 1685819061776
	1685819059472 [label=ReluBackward0]
	1685793494144 -> 1685819059472
	1685793494144 [label=AddmmBackward0]
	1685793497840 -> 1685793494144
	1685818038768 [label="mlp_layers.15.bias
 (8)" fillcolor=lightblue]
	1685818038768 -> 1685793497840
	1685793497840 [label=AccumulateGrad]
	1685793504704 -> 1685793494144
	1685793504704 [label=ReluBackward0]
	1685793491264 -> 1685793504704
	1685793491264 [label=AddmmBackward0]
	1685817099936 -> 1685793491264
	1685818048288 [label="mlp_layers.12.bias
 (16)" fillcolor=lightblue]
	1685818048288 -> 1685817099936
	1685817099936 [label=AccumulateGrad]
	1685817112800 -> 1685793491264
	1685817112800 [label=ReluBackward0]
	1685792453968 -> 1685817112800
	1685792453968 [label=AddmmBackward0]
	1685816495648 -> 1685792453968
	1685818089520 [label="mlp_layers.9.bias
 (32)" fillcolor=lightblue]
	1685818089520 -> 1685816495648
	1685816495648 [label=AccumulateGrad]
	1685816497376 -> 1685792453968
	1685816497376 [label=ReluBackward0]
	1685816499776 -> 1685816497376
	1685816499776 [label=AddmmBackward0]
	1685816492336 -> 1685816499776
	1685818089440 [label="mlp_layers.6.bias
 (64)" fillcolor=lightblue]
	1685818089440 -> 1685816492336
	1685816492336 [label=AccumulateGrad]
	1685816507648 -> 1685816499776
	1685816507648 [label=ReluBackward0]
	1685816500736 -> 1685816507648
	1685816500736 [label=AddmmBackward0]
	1685816503232 -> 1685816500736
	1685818088800 [label="mlp_layers.3.bias
 (128)" fillcolor=lightblue]
	1685818088800 -> 1685816503232
	1685816503232 [label=AccumulateGrad]
	1685816496704 -> 1685816500736
	1685816496704 [label=ReluBackward0]
	1685816498720 -> 1685816496704
	1685816498720 [label=AddmmBackward0]
	1685816497664 -> 1685816498720
	1685818088560 [label="mlp_layers.0.bias
 (256)" fillcolor=lightblue]
	1685818088560 -> 1685816497664
	1685816497664 [label=AccumulateGrad]
	1685816621376 -> 1685816498720
	1685816621376 [label=CatBackward0]
	1685816608128 -> 1685816621376
	1685816608128 [label=EmbeddingBackward0]
	1685816617344 -> 1685816608128
	1685818088880 [label="user_embedding_mlp.weight
 (100, 128)" fillcolor=lightblue]
	1685818088880 -> 1685816617344
	1685816617344 [label=AccumulateGrad]
	1685816610816 -> 1685816621376
	1685816610816 [label=EmbeddingBackward0]
	1685816614848 -> 1685816610816
	1685818089280 [label="item_embedding_mlp.weight
 (200, 128)" fillcolor=lightblue]
	1685818089280 -> 1685816614848
	1685816614848 [label=AccumulateGrad]
	1685816614272 -> 1685816498720
	1685816614272 [label=TBackward0]
	1685816607264 -> 1685816614272
	1685818088720 [label="mlp_layers.0.weight
 (256, 259)" fillcolor=lightblue]
	1685818088720 -> 1685816607264
	1685816607264 [label=AccumulateGrad]
	1685816494304 -> 1685816500736
	1685816494304 [label=TBackward0]
	1685816493728 -> 1685816494304
	1685818088640 [label="mlp_layers.3.weight
 (128, 256)" fillcolor=lightblue]
	1685818088640 -> 1685816493728
	1685816493728 [label=AccumulateGrad]
	1685816502848 -> 1685816499776
	1685816502848 [label=TBackward0]
	1685816499104 -> 1685816502848
	1685818088320 [label="mlp_layers.6.weight
 (64, 128)" fillcolor=lightblue]
	1685818088320 -> 1685816499104
	1685816499104 [label=AccumulateGrad]
	1685816499488 -> 1685792453968
	1685816499488 [label=TBackward0]
	1685816505056 -> 1685816499488
	1685818088960 [label="mlp_layers.9.weight
 (32, 64)" fillcolor=lightblue]
	1685818088960 -> 1685816505056
	1685816505056 [label=AccumulateGrad]
	1685793328960 -> 1685793491264
	1685793328960 [label=TBackward0]
	1685816493632 -> 1685793328960
	1685818048448 [label="mlp_layers.12.weight
 (16, 32)" fillcolor=lightblue]
	1685818048448 -> 1685816493632
	1685816493632 [label=AccumulateGrad]
	1685793502352 -> 1685793494144
	1685793502352 [label=TBackward0]
	1685793327520 -> 1685793502352
	1685818048368 [label="mlp_layers.15.weight
 (8, 16)" fillcolor=lightblue]
	1685818048368 -> 1685793327520
	1685793327520 [label=AccumulateGrad]
	1685819052608 -> 1685818544464
	1685819052608 [label=TBackward0]
	1685819052704 -> 1685819052608
	1685818043808 [label="output_layer.weight
 (1, 136)" fillcolor=lightblue]
	1685818043808 -> 1685819052704
	1685819052704 [label=AccumulateGrad]
	1685818555504 -> 1685817474048
	1685818040208 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1685818544464 -> 1685818040208
	1685818040208 -> 1685817474048 [style=dotted]
}
