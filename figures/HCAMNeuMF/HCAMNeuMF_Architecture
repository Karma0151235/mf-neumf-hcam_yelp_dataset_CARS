digraph {
	graph [size="13.5,13.5"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1690308197376 [label="
 ()" fillcolor=darkolivegreen1]
	1690186434256 [label=SqueezeBackward0]
	1690186443280 -> 1690186434256
	1690186443280 [label=AddmmBackward0]
	1690186439488 -> 1690186443280
	1690308196176 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	1690308196176 -> 1690186439488
	1690186439488 [label=AccumulateGrad]
	1690186446592 -> 1690186443280
	1690186446592 [label=CatBackward0]
	1690186448800 -> 1690186446592
	1690186448800 [label=MulBackward0]
	1690186441792 -> 1690186448800
	1690186441792 [label=EmbeddingBackward0]
	1690186434688 -> 1690186441792
	1690308196336 [label="user_embedding_mf.weight
 (100, 16)" fillcolor=lightblue]
	1690308196336 -> 1690186434688
	1690186434688 [label=AccumulateGrad]
	1690186439680 -> 1690186448800
	1690186439680 [label=EmbeddingBackward0]
	1690186432576 -> 1690186439680
	1690308196256 [label="item_embedding_mf.weight
 (200, 16)" fillcolor=lightblue]
	1690308196256 -> 1690186432576
	1690186432576 [label=AccumulateGrad]
	1690186436656 -> 1690186446592
	1690186436656 [label=ReluBackward0]
	1690186439008 -> 1690186436656
	1690186439008 [label=AddmmBackward0]
	1690186442608 -> 1690186439008
	1690308196656 [label="mlp_layers.6.bias
 (8)" fillcolor=lightblue]
	1690308196656 -> 1690186442608
	1690186442608 [label=AccumulateGrad]
	1690186434304 -> 1690186439008
	1690186434304 [label=ReluBackward0]
	1690186447888 -> 1690186434304
	1690186447888 [label=AddmmBackward0]
	1690185899728 -> 1690186447888
	1690308195936 [label="mlp_layers.3.bias
 (16)" fillcolor=lightblue]
	1690308195936 -> 1690185899728
	1690185899728 [label=AccumulateGrad]
	1690185893344 -> 1690186447888
	1690185893344 [label=ReluBackward0]
	1690185901888 -> 1690185893344
	1690185901888 [label=AddmmBackward0]
	1690185896608 -> 1690185901888
	1690308195776 [label="mlp_layers.0.bias
 (32)" fillcolor=lightblue]
	1690308195776 -> 1690185896608
	1690185896608 [label=AccumulateGrad]
	1690185892528 -> 1690185901888
	1690185892528 [label=CatBackward0]
	1690185901120 -> 1690185892528
	1690185901120 [label=EmbeddingBackward0]
	1690185900928 -> 1690185901120
	1690308196016 [label="user_embedding_mlp.weight
 (100, 16)" fillcolor=lightblue]
	1690308196016 -> 1690185900928
	1690185900928 [label=AccumulateGrad]
	1690185907312 -> 1690185892528
	1690185907312 [label=EmbeddingBackward0]
	1690185902944 -> 1690185907312
	1690308196416 [label="item_embedding_mlp.weight
 (200, 16)" fillcolor=lightblue]
	1690308196416 -> 1690185902944
	1690185902944 [label=AccumulateGrad]
	1690185900208 -> 1690185901888
	1690185900208 [label=TBackward0]
	1690185905872 -> 1690185900208
	1690308195856 [label="mlp_layers.0.weight
 (32, 35)" fillcolor=lightblue]
	1690308195856 -> 1690185905872
	1690185905872 [label=AccumulateGrad]
	1690185906400 -> 1690186447888
	1690185906400 [label=TBackward0]
	1690185902368 -> 1690185906400
	1690308195696 [label="mlp_layers.3.weight
 (16, 32)" fillcolor=lightblue]
	1690308195696 -> 1690185902368
	1690185902368 [label=AccumulateGrad]
	1690186445104 -> 1690186439008
	1690186445104 [label=TBackward0]
	1690185892096 -> 1690186445104
	1690308196096 [label="mlp_layers.6.weight
 (8, 16)" fillcolor=lightblue]
	1690308196096 -> 1690185892096
	1690185892096 [label=AccumulateGrad]
	1690186448656 -> 1690186443280
	1690186448656 [label=TBackward0]
	1690186447504 -> 1690186448656
	1690308196576 [label="output_layer.weight
 (1, 24)" fillcolor=lightblue]
	1690308196576 -> 1690186447504
	1690186447504 [label=AccumulateGrad]
	1690186434256 -> 1690308197376
	1690308197056 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1690186443280 -> 1690308197056
	1690308197056 -> 1690308197376 [style=dotted]
}
