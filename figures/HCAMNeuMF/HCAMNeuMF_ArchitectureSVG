digraph {
	graph [size="13.5,13.5"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1690308226624 [label="
 ()" fillcolor=darkolivegreen1]
	1690186646912 [label=SqueezeBackward0]
	1690186651808 -> 1690186646912
	1690186651808 [label=AddmmBackward0]
	1690186439488 -> 1690186651808
	1690308196176 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	1690308196176 -> 1690186439488
	1690186439488 [label=AccumulateGrad]
	1690186650272 -> 1690186651808
	1690186650272 [label=CatBackward0]
	1690186649744 -> 1690186650272
	1690186649744 [label=MulBackward0]
	1690186653008 -> 1690186649744
	1690186653008 [label=EmbeddingBackward0]
	1690186434688 -> 1690186653008
	1690308196336 [label="user_embedding_mf.weight
 (100, 16)" fillcolor=lightblue]
	1690308196336 -> 1690186434688
	1690186434688 [label=AccumulateGrad]
	1690186656128 -> 1690186649744
	1690186656128 [label=EmbeddingBackward0]
	1690186432576 -> 1690186656128
	1690308196256 [label="item_embedding_mf.weight
 (200, 16)" fillcolor=lightblue]
	1690308196256 -> 1690186432576
	1690186432576 [label=AccumulateGrad]
	1690186660256 -> 1690186650272
	1690186660256 [label=ReluBackward0]
	1690186654688 -> 1690186660256
	1690186654688 [label=AddmmBackward0]
	1690186442608 -> 1690186654688
	1690308196656 [label="mlp_layers.6.bias
 (8)" fillcolor=lightblue]
	1690308196656 -> 1690186442608
	1690186442608 [label=AccumulateGrad]
	1690186651424 -> 1690186654688
	1690186651424 [label=ReluBackward0]
	1690186645712 -> 1690186651424
	1690186645712 [label=AddmmBackward0]
	1690185899728 -> 1690186645712
	1690308195936 [label="mlp_layers.3.bias
 (16)" fillcolor=lightblue]
	1690308195936 -> 1690185899728
	1690185899728 [label=AccumulateGrad]
	1690186647056 -> 1690186645712
	1690186647056 [label=ReluBackward0]
	1690186652720 -> 1690186647056
	1690186652720 [label=AddmmBackward0]
	1690185896608 -> 1690186652720
	1690308195776 [label="mlp_layers.0.bias
 (32)" fillcolor=lightblue]
	1690308195776 -> 1690185896608
	1690185896608 [label=AccumulateGrad]
	1690186646192 -> 1690186652720
	1690186646192 [label=CatBackward0]
	1690186658864 -> 1690186646192
	1690186658864 [label=EmbeddingBackward0]
	1690185900928 -> 1690186658864
	1690308196016 [label="user_embedding_mlp.weight
 (100, 16)" fillcolor=lightblue]
	1690308196016 -> 1690185900928
	1690185900928 [label=AccumulateGrad]
	1690186653824 -> 1690186646192
	1690186653824 [label=EmbeddingBackward0]
	1690185902944 -> 1690186653824
	1690308196416 [label="item_embedding_mlp.weight
 (200, 16)" fillcolor=lightblue]
	1690308196416 -> 1690185902944
	1690185902944 [label=AccumulateGrad]
	1690186657328 -> 1690186652720
	1690186657328 [label=TBackward0]
	1690185905872 -> 1690186657328
	1690308195856 [label="mlp_layers.0.weight
 (32, 35)" fillcolor=lightblue]
	1690308195856 -> 1690185905872
	1690185905872 [label=AccumulateGrad]
	1690186647776 -> 1690186645712
	1690186647776 [label=TBackward0]
	1690185902368 -> 1690186647776
	1690308195696 [label="mlp_layers.3.weight
 (16, 32)" fillcolor=lightblue]
	1690308195696 -> 1690185902368
	1690185902368 [label=AccumulateGrad]
	1690186651616 -> 1690186654688
	1690186651616 [label=TBackward0]
	1690185892096 -> 1690186651616
	1690308196096 [label="mlp_layers.6.weight
 (8, 16)" fillcolor=lightblue]
	1690308196096 -> 1690185892096
	1690185892096 [label=AccumulateGrad]
	1690186652048 -> 1690186651808
	1690186652048 [label=TBackward0]
	1690186447504 -> 1690186652048
	1690308196576 [label="output_layer.weight
 (1, 24)" fillcolor=lightblue]
	1690308196576 -> 1690186447504
	1690186447504 [label=AccumulateGrad]
	1690186646912 -> 1690308226624
	1690308226384 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1690186651808 -> 1690308226384
	1690308226384 -> 1690308226624 [style=dotted]
}
