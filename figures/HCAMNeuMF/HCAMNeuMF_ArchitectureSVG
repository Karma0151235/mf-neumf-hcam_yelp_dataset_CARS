digraph {
	graph [size="19.8,19.8"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1685818182144 [label="
 ()" fillcolor=darkolivegreen1]
	1685816373024 [label=SqueezeBackward0]
	1685816365728 -> 1685816373024
	1685816365728 [label=AddmmBackward0]
	1685819049296 -> 1685816365728
	1685818039968 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	1685818039968 -> 1685819049296
	1685819049296 [label=AccumulateGrad]
	1685816370336 -> 1685816365728
	1685816370336 [label=CatBackward0]
	1685816366112 -> 1685816370336
	1685816366112 [label=MulBackward0]
	1685816364000 -> 1685816366112
	1685816364000 [label=EmbeddingBackward0]
	1685793499952 -> 1685816364000
	1685818089200 [label="user_embedding_mf.weight
 (100, 128)" fillcolor=lightblue]
	1685818089200 -> 1685793499952
	1685793499952 [label=AccumulateGrad]
	1685816365152 -> 1685816366112
	1685816365152 [label=EmbeddingBackward0]
	1685793489008 -> 1685816365152
	1685818089120 [label="item_embedding_mf.weight
 (200, 128)" fillcolor=lightblue]
	1685818089120 -> 1685793489008
	1685793489008 [label=AccumulateGrad]
	1685816366496 -> 1685816370336
	1685816366496 [label=ReluBackward0]
	1685816363616 -> 1685816366496
	1685816363616 [label=AddmmBackward0]
	1685793497840 -> 1685816363616
	1685818038768 [label="mlp_layers.15.bias
 (8)" fillcolor=lightblue]
	1685818038768 -> 1685793497840
	1685793497840 [label=AccumulateGrad]
	1685816363424 -> 1685816363616
	1685816363424 [label=ReluBackward0]
	1685816362848 -> 1685816363424
	1685816362848 [label=AddmmBackward0]
	1685817099936 -> 1685816362848
	1685818048288 [label="mlp_layers.12.bias
 (16)" fillcolor=lightblue]
	1685818048288 -> 1685817099936
	1685817099936 [label=AccumulateGrad]
	1685816362272 -> 1685816362848
	1685816362272 [label=ReluBackward0]
	1685816362080 -> 1685816362272
	1685816362080 [label=AddmmBackward0]
	1685816495648 -> 1685816362080
	1685818089520 [label="mlp_layers.9.bias
 (32)" fillcolor=lightblue]
	1685818089520 -> 1685816495648
	1685816495648 [label=AccumulateGrad]
	1685816361312 -> 1685816362080
	1685816361312 [label=ReluBackward0]
	1685816376768 -> 1685816361312
	1685816376768 [label=AddmmBackward0]
	1685816492336 -> 1685816376768
	1685818089440 [label="mlp_layers.6.bias
 (64)" fillcolor=lightblue]
	1685818089440 -> 1685816492336
	1685816492336 [label=AccumulateGrad]
	1685816376384 -> 1685816376768
	1685816376384 [label=ReluBackward0]
	1685816376480 -> 1685816376384
	1685816376480 [label=AddmmBackward0]
	1685816503232 -> 1685816376480
	1685818088800 [label="mlp_layers.3.bias
 (128)" fillcolor=lightblue]
	1685818088800 -> 1685816503232
	1685816503232 [label=AccumulateGrad]
	1685816365536 -> 1685816376480
	1685816365536 [label=ReluBackward0]
	1685816372448 -> 1685816365536
	1685816372448 [label=AddmmBackward0]
	1685816497664 -> 1685816372448
	1685818088560 [label="mlp_layers.0.bias
 (256)" fillcolor=lightblue]
	1685818088560 -> 1685816497664
	1685816497664 [label=AccumulateGrad]
	1685816366880 -> 1685816372448
	1685816366880 [label=CatBackward0]
	1685816229120 -> 1685816366880
	1685816229120 [label=EmbeddingBackward0]
	1685816617344 -> 1685816229120
	1685818088880 [label="user_embedding_mlp.weight
 (100, 128)" fillcolor=lightblue]
	1685818088880 -> 1685816617344
	1685816617344 [label=AccumulateGrad]
	1685816229312 -> 1685816366880
	1685816229312 [label=EmbeddingBackward0]
	1685816614848 -> 1685816229312
	1685818089280 [label="item_embedding_mlp.weight
 (200, 128)" fillcolor=lightblue]
	1685818089280 -> 1685816614848
	1685816614848 [label=AccumulateGrad]
	1685816229696 -> 1685816372448
	1685816229696 [label=TBackward0]
	1685816607264 -> 1685816229696
	1685818088720 [label="mlp_layers.0.weight
 (256, 259)" fillcolor=lightblue]
	1685818088720 -> 1685816607264
	1685816607264 [label=AccumulateGrad]
	1685816375136 -> 1685816376480
	1685816375136 [label=TBackward0]
	1685816493728 -> 1685816375136
	1685818088640 [label="mlp_layers.3.weight
 (128, 256)" fillcolor=lightblue]
	1685818088640 -> 1685816493728
	1685816493728 [label=AccumulateGrad]
	1685816376576 -> 1685816376768
	1685816376576 [label=TBackward0]
	1685816499104 -> 1685816376576
	1685818088320 [label="mlp_layers.6.weight
 (64, 128)" fillcolor=lightblue]
	1685818088320 -> 1685816499104
	1685816499104 [label=AccumulateGrad]
	1685816361504 -> 1685816362080
	1685816361504 [label=TBackward0]
	1685816505056 -> 1685816361504
	1685818088960 [label="mlp_layers.9.weight
 (32, 64)" fillcolor=lightblue]
	1685818088960 -> 1685816505056
	1685816505056 [label=AccumulateGrad]
	1685816362464 -> 1685816362848
	1685816362464 [label=TBackward0]
	1685816493632 -> 1685816362464
	1685818048448 [label="mlp_layers.12.weight
 (16, 32)" fillcolor=lightblue]
	1685818048448 -> 1685816493632
	1685816493632 [label=AccumulateGrad]
	1685816363808 -> 1685816363616
	1685816363808 [label=TBackward0]
	1685793327520 -> 1685816363808
	1685818048368 [label="mlp_layers.15.weight
 (8, 16)" fillcolor=lightblue]
	1685818048368 -> 1685793327520
	1685793327520 [label=AccumulateGrad]
	1685816366688 -> 1685816365728
	1685816366688 [label=TBackward0]
	1685819052704 -> 1685816366688
	1685818043808 [label="output_layer.weight
 (1, 136)" fillcolor=lightblue]
	1685818043808 -> 1685819052704
	1685819052704 [label=AccumulateGrad]
	1685816373024 -> 1685818182144
	1685818181824 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	1685816365728 -> 1685818181824
	1685818181824 -> 1685818182144 [style=dotted]
}
